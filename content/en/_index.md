---
title: May.la
type: docs
---

This is Philip May's page. It is about the following topics:

- [My Blog](/posts/)
- [IT](/docs/it/)
- [Python](/docs/python/)
- [Machine Learning](/docs/machine-learning/)
- [Linux](/docs/linux/)

# News about My Work
- **March 07, 2022:** Based on our [GC4 text corpus](https://german-nlp-group.github.io/projects/gc4-corpus.html) we open sourced a new German T5 model: [t5-efficient-gc4-german-base-nl36](https://huggingface.co/GermanT5/t5-efficient-gc4-german-base-nl36)
- **July 12, 2021:** As part of my work for Deutsche Telekom, I published the first German and English summarization model: [deutsche-telekom/mt5-small-sum-de-en-v1](https://huggingface.co/deutsche-telekom/mt5-small-sum-de-en-v1)
- **April 18, 2021:** [German NLP Group](https://german-nlp-group.github.io/) releases [The German colossal, cleaned Common Crawl corpus (GC4 corpus)](https://german-nlp-group.github.io/projects/gc4-corpus.html). A preprocessed (450 GB zipped) German text corpus based on [Common Crawl](https://commoncrawl.org/). Many thanks to [iisys](https://www.iisys.de/) (the Institute of Information Systems Hof University) for hosting this dataset.
- **April 5, 2021:** Published training code for cross-lingual [sentence-transformers](https://github.com/UKPLab/sentence-transformers) models: [German-NLP-Group/xlsr](https://github.com/German-NLP-Group/xlsr)
- **March 29, 2021:** Adding the machine translated multilingual STS benchmark (STSb) dataset called *stsb_multi_mt* as a HuggingFace dataset: https://huggingface.co/datasets/stsb_multi_mt
- **March 21, 2021:** Started website for *German NLP Group* as a GitHub Page: https://german-nlp-group.github.io/
- **March 20, 2021:** Published the *machine translated multilingual STS benchmark (STSb) dataset* under open-source license: [PhilipMay/stsb-multi-mt](https://github.com/PhilipMay/stsb-multi-mt)
- **February 22, 2021:** Together with [Philipp Reissel](https://twitter.com/phil_ipp_) from [ambeRoad](https://amberoad.de/) I released the 2nd version of our *German Electra NLP language model*. Training is continued for an additional 734,000 steps to a total of 1,500,000 steps. An extensive evaluation on the GermEval18 Coarse dataset shows that this is the best performing German language model of its size: [german-nlp-group/electra-base-german-uncased](https://huggingface.co/german-nlp-group/electra-base-german-uncased)
- **December 1, 2020:** Together with [Philipp Reissel](https://twitter.com/phil_ipp_) from [ambeRoad](https://amberoad.de/) I gave a talk about the training and evaluation of our open-source [German Electra NLP language model](https://huggingface.co/german-nlp-group/electra-base-german-uncased). Here is the recording on YouTube: https://www.youtube.com/watch?v=cxgrTd2AQis and the {download}`slides <_static/doc/German_Language_Model_Training_and_Evaluation.pdf>`.
- **October 24, 2020:** Published a *cross-lingual model for English and German semantic sentence embeddings* under open-source license: [T-Systems-onsite/cross-en-de-roberta-sentence-transformer](https://huggingface.co/T-Systems-onsite/cross-en-de-roberta-sentence-transformer)
- **August 18, 2020:** Together with [Philipp Reissel](https://twitter.com/phil_ipp_) from [ambeRoad](https://amberoad.de/) I released a new open-source *German Electra NLP language model*: [german-nlp-group/electra-base-german-uncased](https://huggingface.co/german-nlp-group/electra-base-german-uncased)
- **April 7, 2020** Started to maintain the *python-configparser* [Arch Linux](https://archlinux.org/) AUR package: [python-configparser](https://aur.archlinux.org/packages/python-configparser/)
- **March 14, 2020:** Started to maintain the *PyCharm IDE* [Arch Linux](https://archlinux.org/) AUR package: [pycharm-community-jre](https://aur.archlinux.org/packages/pycharm-community-jre/)
- **September 22, 2019:** On arXiv.org I publiched my first paper: [Improved Image Augmentation for Convolutional Neural Networks by Copyout and CopyPairing](https://arxiv.org/abs/1909.00390)
- **September 9, 2018:** Started to maintain the [conda-forge](https://conda-forge.org/) *hyperopt package*: [hyperopt-feedstock](https://github.com/conda-forge/hyperopt-feedstock)

## Feedback and Questions
If you have questions or comments about this content please feel free to
open a [GitHub issue](https://github.com/PhilipMay/may-la-hugo/issues/new).

## Profiles
- [LinkedIn](https://www.linkedin.com/in/philip-may-3992889a/)
- [Xing](https://www.xing.com/profile/Philip_May)
- [Twitter](https://twitter.com/pMay)
- [GitHub](https://github.com/PhilipMay)
- [GitLab](https://gitlab.com/PhilipMay)
- [PyPI Projects](https://pypi.org/user/Dieshe/)
- [Google Scholar](https://scholar.google.de/citations?user=tmsgMY8AAAAJ&hl=de&oi=sra)
- ORCID: <https://orcid.org/0000-0002-2154-3421>

[![Build & Deploy May.la](https://github.com/PhilipMay/may-la-hugo/actions/workflows/make-deploy.yml/badge.svg)](https://github.com/PhilipMay/may-la-hugo/actions/workflows/make-deploy.yml)
